# RetailFlow â€” Enterprise Retail Data Platform

Production-grade data platform for a retail company, built on **Azure Databricks** with a **medallion architecture** (RAW â†’ BRONZE â†’ SILVER â†’ GOLD). Processes online orders, store sales, product catalog, inventory, customers, payments, and clickstream events for analytics, BI, and reporting.

> ðŸš§ **This project is under active development and continuously evolving.** The current state already reflects production-oriented design decisions.

---

## Repository structure

```
RetailFlow/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ environments/          # dev, stg, prod config (YAML)
â”‚   â””â”€â”€ schemas/               # Raw schema references (JSON)
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ raw/               # Ingestion: orders, customers, products, inventory, clickstream
â”‚   â”‚   â”œâ”€â”€ bronze/            # Schema enforcement, audit columns, Delta
â”‚   â”‚   â”œâ”€â”€ silver/            # Clean, dedup, validation
â”‚   â”‚   â”œâ”€â”€ gold/              # fact_orders, fact_sales, dim_customer (SCD2), dim_product, daily_revenue_mart
â”‚   â”‚   â””â”€â”€ observability/     # Job monitoring, logging
â”‚   â””â”€â”€ jobs/                  # Job definition JSON (RetailFlow_Main_Pipeline)
â”œâ”€â”€ dlt/
â”‚   â””â”€â”€ pipelines/             # Delta Live Tables: Bronze + Silver (orders)
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/                  # Optional: medallion DAG (trigger Databricks job)
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ retailflow/            # Optional: marts (e.g. daily_revenue)
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ databricks/        # Workspace
â”‚   â”‚   â”œâ”€â”€ storage/          # ADLS Gen2 (raw, processed)
â”‚   â”‚   â”œâ”€â”€ key_vault/        # Secrets
â”‚   â”‚   â””â”€â”€ networking/       # VNet placeholders
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ terraform.tfvars.example
â”œâ”€â”€ scripts/                   # Bootstrap RAW folders, secret scope
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DATA_FLOW.md
â”‚   â”œâ”€â”€ RAW_LAYER_DESIGN.md
â”‚   â”œâ”€â”€ UNITY_CATALOG.md
â”‚   â””â”€â”€ OBSERVABILITY.md
â”œâ”€â”€ .github/workflows/         # CI/CD: deploy notebooks, jobs, promote env, tests
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Architecture (summary)

- **RAW:** ADLS Gen2; immutable; partition by `ingestion_date`; JSON/CSV/Parquet as received. Supports replay and schema evolution.
- **Bronze:** Delta in Unity Catalog; minimal parsing, flatten JSON, audit columns (`_ingestion_ts`, `_source_file`).
- **Silver:** Delta; cleaned, deduplicated, validated, business keys.
- **Gold:** Delta; reporting-ready: `fact_sales`, `fact_orders`, `dim_customer` (SCD2), `dim_product`, store dimension, inventory snapshot, `daily_revenue_mart`.

Tech: **Azure Databricks**, **Delta Lake**, **Unity Catalog**, **ADLS Gen2**, **Azure Key Vault**, **Azure Monitor**, **Terraform**, **GitHub Actions**, optional **Airflow** and **dbt**.

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md), [docs/DATA_FLOW.md](docs/DATA_FLOW.md), [docs/RAW_LAYER_DESIGN.md](docs/RAW_LAYER_DESIGN.md).

---

## Data flow

1. **Ingestion** (notebooks) â†’ RAW on ADLS (`/data/raw/orders`, `customers`, `products`, `inventory`, `clickstream`, etc.).
2. **Bronze** notebooks/DLT read RAW â†’ parse, add audit columns â†’ Delta tables in Bronze schema.
3. **Silver** reads Bronze â†’ clean, dedupe, validate â†’ Delta in Silver schema.
4. **Gold** reads Silver â†’ facts, dimensions, marts â†’ Delta in Gold schema.
5. **Consumption:** Power BI, Tableau, Databricks SQL on Gold (and Silver as needed).

---

## RAW layer design

- **Immutability:** Append-only; no updates/deletes.
- **Exact copy:** No transformations; store payload as-is.
- **Partitioning:** `ingestion_date=YYYY-MM-DD`.
- **Replay:** Reprocess any date range from RAW without changing RAW.
- **Metadata:** Ingestion timestamp and source file captured in Bronze when reading RAW.

Details: [docs/RAW_LAYER_DESIGN.md](docs/RAW_LAYER_DESIGN.md).

---

## Example notebook (RAW orders)

See [databricks/notebooks/raw/01_ingest_orders_api.py](databricks/notebooks/raw/01_ingest_orders_api.py): fetches from Orders API, writes one JSON file per run under `raw_base/ingestion_date=YYYY-MM-DD/batch_<id>.json`. No transforms.

---

## Example job config

Main pipeline job: [databricks/jobs/retailflow_main_job.json](databricks/jobs/retailflow_main_job.json). Tasks: ingest RAW (orders, customers) â†’ Bronze â†’ Silver â†’ Gold (fact_orders, dim_customer, daily_revenue_mart). Schedule: daily 02:00 UTC. Concurrency: 1. Cluster: 14.3.x, 2 workers, AQE and Delta optimize enabled.

---

## Terraform

- **Root:** `main.tf` wires resource group, Databricks module, storage, Key Vault, optional networking.
- **Modules:** `databricks` (workspace), `storage` (ADLS Gen2, containers `raw`/`processed`), `key_vault`, `networking` (VNet/subnets).
- **Backend:** Configure `backend "azurerm"` (e.g. state in Azure Storage).
- **Environments:** Use `terraform.tfvars` or workspaces for dev/stg/prod; see `terraform.tfvars.example`.

```bash
cd terraform
terraform init
terraform plan -var="environment=dev"
terraform apply -var="environment=dev"
```

---

## CI/CD (GitHub Actions)

- **deploy-notebooks.yml:** On push to `main` (databricks/notebooks, dlt) â€” sync notebooks (e.g. via Repos).
- **deploy-jobs.yml:** On push to job definitions â€” deploy/update Databricks jobs via API.
- **promote-environment.yml:** Manual; promote to stg or prod (config + optional Terraform).
- **tests.yml:** Pytest unit tests + Ruff lint on push/PR.

Secrets: `DATABRICKS_HOST`, `DATABRICKS_TOKEN`; for Terraform: `ARM_*`.

---

## Unity Catalog and secrets

- Catalogs: `retailflow_dev`, `retailflow_stg`, `retailflow_prod` with schemas `raw`, `bronze`, `silver`, `gold`.
- Roles: raw_ingestion, bronze_reader, silver_reader, gold_reader, analytics, platform_admin.
- Secret scope: Azure Key Vaultâ€“backed scope (e.g. `retailflow-keyvault`) for API keys and DB connection strings.

See [docs/UNITY_CATALOG.md](docs/UNITY_CATALOG.md).

---

## Observability

- Logging and job run metadata: [databricks/notebooks/observability/job_monitor.py](databricks/notebooks/observability/job_monitor.py).
- Alerts: Databricks job failure emails; optional Azure Monitor.
- Data quality: DLT expectations in [dlt/pipelines/bronze_silver_dlt.py](dlt/pipelines/bronze_silver_dlt.py).

See [docs/OBSERVABILITY.md](docs/OBSERVABILITY.md).

---

## Next steps

See [docs/NEXT_STEPS.md](docs/NEXT_STEPS.md) for implementation checklist: Terraform apply, secret scope, RAW bootstrap, run jobs, DLT, dbt, and monitoring.

---

## License

Internal / portfolio use. Adjust org names, endpoints, and secrets for your environment.
